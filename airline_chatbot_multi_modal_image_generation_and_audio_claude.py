# -*- coding: utf-8 -*-
"""Airline_Chatbot-Multi_Modal_Image_Generation_and_Audio_Claude.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16UjIkLJCOVvEWQ-h8rUPsLRihOa47Cvx
"""

!pip install python-dotenv

!pip install anthropic

!pip install gradio

# Imports
import os
import requests
from bs4 import BeautifulSoup
from typing import List
from dotenv import load_dotenv
import anthropic
import json
import base64
from io import BytesIO
from PIL import Image
from pydub import AudioSegment
import gradio as gr                           # Import Gradio for UI
from IPython.display import Audio, display

# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

load_dotenv(override=True)
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")

if anthropic_api_key:
    print(f"Anthropic API Key exists and begins {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")

claude = anthropic.Anthropic(api_key=anthropic_api_key)

# System behavior
system_message = (
    "You are a helpful assistant for an airline called FlightAI. "
    "Give short, courteous answers, no more than 1 sentence. "
    "Always be accurate. If you don't know the answer, say so."
)

# Mock ticket prices
ticket_prices = {"london": "$799", "paris": "$899", "tokyo": "$1400", "berlin": "$499"}

def get_ticket_price(destination_city):
    city = destination_city.lower()
    return ticket_prices.get(city, "Unknown")

# Detect city from user message
def detect_city_from_text(text):
    text = text.lower()
    for city in ticket_prices:
        if city in text:
            return city
    return None

# DALL-E image generator (still uses OpenAI)
from openai import OpenAI
openai = OpenAI()
def artist(city):
    response = openai.images.generate(
        model="dall-e-3",
        prompt=f"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style",
        size="1024x1024",
        n=1,
        response_format="b64_json",
    )
    image_base64 = response.data[0].b64_json
    image_data = base64.b64decode(image_base64)
    return Image.open(BytesIO(image_data))

# Text-to-speech with OpenAI
def talker(message):
    response = openai.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=message
    )
    audio_stream = BytesIO(response.content)
    output_filename = "output_audio.mp3"
    with open(output_filename, "wb") as f:
        f.write(audio_stream.read())
    return output_filename

# Convert chat history for Claude
def convert_history_to_prompt(history):
    prompt = ""
    for turn in history:
        if turn["role"] == "user":
            prompt += f"\nHuman: {turn['content']}"
        else:
            prompt += f"\nAssistant: {turn['content']}"
    prompt += "\nAssistant:"
    return prompt.strip()

# Claude-based chatbot logic
def chat_claude(history):
    messages = [{"role": "user", "content": convert_history_to_prompt(history)}]
    response = claude.messages.create(
        model="claude-3-haiku-20240307",
        system=system_message,
        max_tokens=1000,
        temperature=0.7,
        messages=messages
    )

    reply = response.content[0].text
    image = None
    audio_path = None

    # Simulate tool call
    city = detect_city_from_text(reply)
    if city:
        price = get_ticket_price(city)
        reply = f"The price to {city.title()} is {price}."
        image = artist(city)

    audio_path = talker(reply)
    history.append({"role": "assistant", "content": reply})

    return history, image, audio_path

# Gradio UI
with gr.Blocks() as ui:
    with gr.Row():
        chatbot = gr.Chatbot(height=500, type="messages")
        image_output = gr.Image(height=500)
        audio_output = gr.Audio(label="Voice", autoplay=True)
    with gr.Row():
        entry = gr.Textbox(label="Chat with FlightAI:")
    with gr.Row():
        clear = gr.Button("Clear")

    def do_entry(message, history):
        history += [{"role": "user", "content": message}]
        return "", history

    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(
        chat_claude, inputs=chatbot, outputs=[chatbot, image_output, audio_output]
    )
    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)

ui.launch(inbrowser=True)





"""**Lets go Multi-Modal!**

We can use DALL-E-3, the image generation model behind GPT-4o, to make us some images
"""

# Some imports for handling images

import base64
from io import BytesIO
from PIL import Image           #Python Image Library

def artist(city):
    image_response = openai.images.generate(
            model="dall-e-3",                       #Dall-e-3 model
            prompt=f"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style", #prompt to display kind of image
            size="1024x1024",                       #size of image
            n=1,                                    #number of images to generate
            response_format="b64_json",              #format of response
        )
    image_base64 = image_response.data[0].b64_json
    image_data = base64.b64decode(image_base64)          #Decode image to bytes
    return Image.open(BytesIO(image_data))

image = artist("New York City")
display(image)

"""Let's make a function talker that uses **OpenAI's speech model** to generate Audio"""

from pydub import AudioSegment
from pydub.playback import play

import base64
from io import BytesIO
from PIL import Image
from IPython.display import Audio, display

# def talker(message):
#     response = openai.audio.speech.create(
#         model="tts-1",
#         voice="alloy",                          #onxy is another sound
#         input=message)

#     audio_stream = BytesIO(response.content)
#     output_filename = "output_audio.mp3"
#     with open(output_filename, "wb") as f:
#         f.write(audio_stream.read())

#     # Play the generated audio
#     display(Audio(output_filename, autoplay=True))

# talker("Well, hi there")


def talker(message):
    response = openai.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=message
    )
    audio_stream = BytesIO(response.content)
    output_filename = "output_audio.mp3"
    with open(output_filename, "wb") as f:
        f.write(audio_stream.read())

    return output_filename  # Return path for Gradio

"""**Agent Framework**"""

# def chat(history):                                                                              #usual chat function that takes message and history for OpenAi and calls response
#     messages = [{"role": "system", "content": system_message}] + history
#     response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)
#     image = None

#     if response.choices[0].finish_reason=="tool_calls":                                         #use of tools
#         message = response.choices[0].message
#         response, city = handle_tool_call(message)
#         messages.append(message)
#         messages.append(response)
#         image = artist(city)                                                                    #image generation - if model runs tool to find price of ticket, then image of city will be generated
#         response = openai.chat.completions.create(model=MODEL, messages=messages)

#     reply = response.choices[0].message.content
#     history += [{"role":"assistant", "content":reply}]

#     # Comment out or delete the next line if you'd rather skip Audio for now..
#     talker(reply)                                                                              #audio generation - speak response

#     return history, image


def chat(history):
    messages = [{"role": "system", "content": system_message}] + history
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)
    image = None

    if response.choices[0].finish_reason == "tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        image = artist(city)
        response = openai.chat.completions.create(model=MODEL, messages=messages)

    reply = response.choices[0].message.content
    history += [{"role": "assistant", "content": reply}]

    audio_path = talker(reply)  # generate and return audio

    return history, image, audio_path

# More involved Gradio code as we're not using the preset Chat interface!
# Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.

# with gr.Blocks() as ui:
#     with gr.Row():
#         chatbot = gr.Chatbot(height=500, type="messages")
#         image_output = gr.Image(height=500)
#     with gr.Row():
#         entry = gr.Textbox(label="Chat with our AI Assistant:")
#     with gr.Row():
#         clear = gr.Button("Clear")

#     def do_entry(message, history):
#         history += [{"role":"user", "content":message}]
#         return "", history

#     entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(
#         chat, inputs=chatbot, outputs=[chatbot, image_output]
#     )
#     clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)

# ui.launch(inbrowser=True)


with gr.Blocks() as ui:
    with gr.Row():
        chatbot = gr.Chatbot(height=500, type="messages")
        image_output = gr.Image(height=500)
        audio_output = gr.Audio(label="AI Voice Response", autoplay=True)  # NEW!
    with gr.Row():
        entry = gr.Textbox(label="Chat with our AI Assistant:")
    with gr.Row():
        clear = gr.Button("Clear")

    def do_entry(message, history):
        history += [{"role": "user", "content": message}]
        return "", history

    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(
    chat_claude, inputs=chatbot, outputs=[chatbot, image_output, audio_output]
    )
    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)

ui.launch(inbrowser=True)